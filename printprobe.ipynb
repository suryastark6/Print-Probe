{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "uYJYCOD2nAhH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import glob\n",
        "import xml.etree.ElementTree as ET\n",
        "import torch\n",
        "from torchvision.models.detection import fasterrcnn_resnet50_fpn\n",
        "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "\n",
        "# Define your dataset class\n",
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, image_folder, annotation_folder, transform=None):\n",
        "        self.image_folder = image_folder\n",
        "        self.annotation_folder = annotation_folder\n",
        "        self.transform = transform\n",
        "\n",
        "        # Get paths for images and annotations\n",
        "        self.image_paths = glob.glob(os.path.join(image_folder, '*.jpg'))\n",
        "        self.annotation_paths = [os.path.join(annotation_folder, os.path.basename(p).replace('.jpg', '.xml')) for p in self.image_paths]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_paths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image_path = self.image_paths[idx]\n",
        "        annotation_path = self.annotation_paths[idx]\n",
        "\n",
        "        # Load image\n",
        "        image = Image.open(image_path).convert(\"RGB\")\n",
        "\n",
        "        # Load annotations\n",
        "        boxes = self.parse_xml(annotation_path)\n",
        "\n",
        "        # Filter out invalid bounding boxes\n",
        "        boxes = [box for box in boxes if self.is_valid_box(box)]\n",
        "\n",
        "        # Convert bounding box coordinates to tensors\n",
        "        boxes = torch.as_tensor(boxes, dtype=torch.float32)\n",
        "\n",
        "        # If there are no valid annotations, create dummy labels\n",
        "        if len(boxes) == 0:\n",
        "            boxes = torch.zeros((1, 4), dtype=torch.float32)\n",
        "            labels = torch.tensor([0], dtype=torch.int64)  # Background class label\n",
        "        else:\n",
        "            labels = torch.ones(boxes.shape[0], dtype=torch.int64)  # Spaghetti class label\n",
        "\n",
        "        target = {}\n",
        "        target[\"boxes\"] = boxes\n",
        "        target[\"labels\"] = labels\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        return image, target\n",
        "\n",
        "\n",
        "    def parse_xml(self, xml_path):\n",
        "        tree = ET.parse(xml_path)\n",
        "        root = tree.getroot()\n",
        "        boxes = []\n",
        "        for obj in root.findall('object'):\n",
        "            bbox = obj.find('bndbox')\n",
        "            if bbox is not None:\n",
        "                xmin = int(bbox.find('xmin').text)\n",
        "                ymin = int(bbox.find('ymin').text)\n",
        "                xmax = int(bbox.find('xmax').text)\n",
        "                ymax = int(bbox.find('ymax').text)\n",
        "                boxes.append([xmin, ymin, xmax, ymax])\n",
        "        return boxes\n",
        "\n",
        "    def is_valid_box(self, box):\n",
        "        xmin, ymin, xmax, ymax = box\n",
        "        return xmax > xmin and ymax > ymin\n",
        "\n",
        "# Define transforms\n",
        "transform = transforms.Compose([transforms.ToTensor()])\n",
        "\n",
        "# Initialize dataset\n",
        "image_folder = \"/content/drive/MyDrive/print fails.v1i.voc/train/img/\"\n",
        "annotation_folder = \"/content/drive/MyDrive/print fails.v1i.voc/train/annot/\"\n",
        "train_dataset = CustomDataset(image_folder=image_folder, annotation_folder=annotation_folder, transform=transform)\n",
        "\n",
        "# Define batch size\n",
        "batch_size = 2  # Adjust as needed\n",
        "\n",
        "# Initialize dataloader with custom collate_fn\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=lambda x: tuple(zip(*x)))\n",
        "\n",
        "# Define model\n",
        "model = fasterrcnn_resnet50_fpn(pretrained=True)\n",
        "num_classes = 2  # Only one class (spaghetti)\n",
        "in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
        "model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
        "\n",
        "# Set device\n",
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "model.to(device)\n",
        "\n",
        "# Define optimizer and learning rate scheduler\n",
        "params = [p for p in model.parameters() if p.requires_grad]\n",
        "optimizer = torch.optim.SGD(params, lr=0.005, momentum=0.9, weight_decay=0.0005)\n",
        "lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.1)\n",
        "\n",
        "# Train the model\n",
        "num_epochs = 10  # Adjust as needed\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    for images, targets in train_loader:\n",
        "        images = list(image.to(device) for image in images)\n",
        "        targets = [{k: v.to(device) for k, v in target.items()} for target in targets]  # Ensure targets are on the same device as images\n",
        "        optimizer.zero_grad()\n",
        "        loss_dict = model(images, targets)\n",
        "        losses = sum(loss for loss in loss_dict.values())\n",
        "        losses.backward()\n",
        "        optimizer.step()\n",
        "    print(f\"Epoch [{epoch + 1}/{num_epochs}], Loss: {losses.item()}\")\n",
        "\n",
        "# Save the trained model\n",
        "torch.save(model.state_dict(), 'spaghetti_detection_model.pth')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kykL3PGk5R4_",
        "outputId": "21866ffb-9199-42b8-b498-f8cfa7b5a426"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=FasterRCNN_ResNet50_FPN_Weights.COCO_V1`. You can also use `weights=FasterRCNN_ResNet50_FPN_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/fasterrcnn_resnet50_fpn_coco-258fb6c6.pth\" to /root/.cache/torch/hub/checkpoints/fasterrcnn_resnet50_fpn_coco-258fb6c6.pth\n",
            "100%|██████████| 160M/160M [00:02<00:00, 83.3MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/10], Loss: 0.5320544242858887\n",
            "Epoch [2/10], Loss: 0.8433959484100342\n",
            "Epoch [3/10], Loss: 0.6781300902366638\n",
            "Epoch [4/10], Loss: 0.47257164120674133\n",
            "Epoch [5/10], Loss: 0.6237234473228455\n",
            "Epoch [6/10], Loss: 0.649628758430481\n",
            "Epoch [7/10], Loss: 0.40053001046180725\n",
            "Epoch [8/10], Loss: 0.2242920994758606\n",
            "Epoch [9/10], Loss: 0.3859308958053589\n",
            "Epoch [10/10], Loss: 0.24816091358661652\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import glob\n",
        "import xml.etree.ElementTree as ET\n",
        "import torch\n",
        "from torchvision.models.detection import fasterrcnn_resnet50_fpn\n",
        "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "\n",
        "# Define your dataset class\n",
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, image_folder, annotation_folder, transform=None):\n",
        "        self.image_folder = image_folder\n",
        "        self.annotation_folder = annotation_folder\n",
        "        self.transform = transform\n",
        "\n",
        "        # Get paths for images and annotations\n",
        "        self.image_paths = glob.glob(os.path.join(image_folder, '*.jpg'))\n",
        "        self.annotation_paths = [os.path.join(annotation_folder, os.path.basename(p).replace('.jpg', '.xml')) for p in self.image_paths]\n",
        "\n",
        "        # Filter out samples with invalid bounding boxes\n",
        "        print(\"Filtering out samples with invalid bounding boxes...\")\n",
        "        self.valid_samples = []\n",
        "        for image_path, annotation_path in zip(self.image_paths, self.annotation_paths):\n",
        "            boxes = self.parse_xml(annotation_path)\n",
        "            if self.has_valid_boxes(boxes):\n",
        "                self.valid_samples.append((image_path, annotation_path))\n",
        "        print(f\"Total valid samples: {len(self.valid_samples)}\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.valid_samples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image_path, annotation_path = self.valid_samples[idx]\n",
        "\n",
        "        # Load image\n",
        "        print(f\"Loading image: {image_path}\")\n",
        "        image = Image.open(image_path).convert(\"RGB\")\n",
        "\n",
        "        # Load annotations\n",
        "        print(f\"Loading annotations: {annotation_path}\")\n",
        "        boxes = self.parse_xml(annotation_path)\n",
        "\n",
        "        # Convert bounding box coordinates to tensors\n",
        "        boxes = torch.as_tensor(boxes, dtype=torch.float32)\n",
        "\n",
        "        # Create dummy labels\n",
        "        labels = torch.ones(boxes.shape[0], dtype=torch.int64)  # Spaghetti class label\n",
        "\n",
        "        target = {}\n",
        "        target[\"boxes\"] = boxes\n",
        "        target[\"labels\"] = labels\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        return image, target\n",
        "\n",
        "    def parse_xml(self, xml_path):\n",
        "        tree = ET.parse(xml_path)\n",
        "        root = tree.getroot()\n",
        "        boxes = []\n",
        "        for obj in root.findall('object'):\n",
        "            bbox = obj.find('bndbox')\n",
        "            if bbox is not None:\n",
        "                xmin = int(bbox.find('xmin').text)\n",
        "                ymin = int(bbox.find('ymin').text)\n",
        "                xmax = int(bbox.find('xmax').text)\n",
        "                ymax = int(bbox.find('ymax').text)\n",
        "                boxes.append([xmin, ymin, xmax, ymax])\n",
        "        return boxes\n",
        "\n",
        "    def has_valid_boxes(self, boxes):\n",
        "        return any(box[2] > box[0] and box[3] > box[1] for box in boxes)\n",
        "\n",
        "# Define transforms\n",
        "transform = transforms.Compose([transforms.ToTensor()])\n",
        "\n",
        "# Initialize dataset\n",
        "print(\"Initializing dataset...\")\n",
        "image_folder = \"/content/drive/MyDrive/prob_dataset/train/img/\"\n",
        "annotation_folder = \"/content/drive/MyDrive/prob_dataset/train/annot/\"\n",
        "train_dataset = CustomDataset(image_folder=image_folder, annotation_folder=annotation_folder, transform=transform)\n",
        "\n",
        "# Define batch size\n",
        "batch_size = 2  # Adjust as needed\n",
        "\n",
        "# Initialize dataloader with custom collate_fn\n",
        "print(\"Initializing dataloader...\")\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=lambda x: tuple(zip(*x)))\n",
        "\n",
        "# Define model\n",
        "print(\"Defining model...\")\n",
        "model = fasterrcnn_resnet50_fpn(pretrained=True)\n",
        "num_classes = 2  # Only one class (spaghetti)\n",
        "in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
        "model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
        "\n",
        "# Set device\n",
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "model.to(device)\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Define optimizer and learning rate scheduler\n",
        "print(\"Defining optimizer and learning rate scheduler...\")\n",
        "params = [p for p in model.parameters() if p.requires_grad]\n",
        "optimizer = torch.optim.SGD(params, lr=0.005, momentum=0.9, weight_decay=0.0005)\n",
        "lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.1)\n",
        "\n",
        "# Train the model\n",
        "num_epochs = 10  # Adjust as needed\n",
        "print(\"Training the model...\")\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    for images, targets in train_loader:\n",
        "        images = list(image.to(device) for image in images)\n",
        "        targets = [{k: v.to(device) for k, v in target.items()} for target in targets]  # Ensure targets are on the same device as images\n",
        "        optimizer.zero_grad()\n",
        "        loss_dict = model(images, targets)\n",
        "        losses = sum(loss for loss in loss_dict.values())\n",
        "        losses.backward()\n",
        "        optimizer.step()\n",
        "    print(f\"Epoch [{epoch + 1}/{num_epochs}], Loss: {losses.item()}\")\n",
        "\n",
        "# Save the trained model\n",
        "print(\"Saving the trained model...\")\n",
        "torch.save(model.state_dict(), '/content/drive/MyDrive/prob_dataset/spaghetti_detection_model.pth')\n",
        "\n",
        "print(\"Training completed.\")\n"
      ],
      "metadata": {
        "id": "JZnkR1D77bfp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchvision.transforms as T\n",
        "from torchvision.models.detection import fasterrcnn_resnet50_fpn\n",
        "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
        "import cv2\n",
        "\n",
        "# Load the trained model\n",
        "model = fasterrcnn_resnet50_fpn(pretrained=False)\n",
        "num_classes = 2  # Background + Spaghetti\n",
        "in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
        "model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
        "model.load_state_dict(torch.load('/content/drive/MyDrive/print fails.v1i.voc/train/spaghetti_detection_model.pth'))\n",
        "model.eval()\n",
        "\n",
        "# Define the transform to preprocess the image\n",
        "transform = T.Compose([T.ToTensor()])\n",
        "\n",
        "# Define the function to perform inference and draw bounding boxes\n",
        "def predict(frame, prev_boxes=None, alpha=0.7):\n",
        "    image_tensor = transform(frame).unsqueeze(0)\n",
        "\n",
        "    # Perform inference\n",
        "    with torch.no_grad():\n",
        "        prediction = model(image_tensor)[0]\n",
        "\n",
        "    # Get bounding boxes from the prediction\n",
        "    current_boxes = prediction['boxes'].cpu().numpy().astype(int)\n",
        "\n",
        "    # Apply temporal smoothing\n",
        "    if prev_boxes is not None and len(prev_boxes) > 0:\n",
        "        smoothed_boxes = alpha * current_boxes + (1 - alpha) * prev_boxes\n",
        "        smoothed_boxes = smoothed_boxes.astype(int)\n",
        "        for box in smoothed_boxes:\n",
        "            xmin, ymin, xmax, ymax = box\n",
        "            cv2.rectangle(frame, (xmin, ymin), (xmax, ymax), (0, 0, 255), 2)\n",
        "    else:\n",
        "        for box in current_boxes:\n",
        "            xmin, ymin, xmax, ymax = box\n",
        "            cv2.rectangle(frame, (xmin, ymin), (xmax, ymax), (0, 0, 255), 2)\n",
        "        smoothed_boxes = current_boxes\n",
        "\n",
        "    return frame, smoothed_boxes\n",
        "\n",
        "\n",
        "# Define input and output video paths\n",
        "input_video_path = \"/content/drive/MyDrive/print fails.v1i.voc/3dprintfail.mp4\"\n",
        "output_video_path = \"/content/drive/MyDrive/print fails.v1i.voc/detected_with_temporal_smoothing.mp4\"\n",
        "\n",
        "# Open the input video file\n",
        "cap = cv2.VideoCapture(input_video_path)\n",
        "\n",
        "# Get video properties\n",
        "frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "frame_rate = cap.get(cv2.CAP_PROP_FPS)\n",
        "\n",
        "# Create VideoWriter object to save the output video\n",
        "fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
        "out = cv2.VideoWriter(output_video_path, fourcc, frame_rate, (frame_width, frame_height))\n",
        "\n",
        "# Process the first frame to initialize previous_boxes\n",
        "ret, frame = cap.read()\n",
        "prev_boxes = None\n",
        "\n",
        "# Process each frame of the video\n",
        "frame_count = 0\n",
        "while ret:\n",
        "    frame_with_boxes, prev_boxes = predict(frame, prev_boxes)\n",
        "    out.write(frame_with_boxes)\n",
        "    frame_count += 1\n",
        "    print(f\"Processed frame {frame_count}\")\n",
        "    ret, frame = cap.read()\n",
        "\n",
        "# Release video objects\n",
        "cap.release()\n",
        "out.release()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 367
        },
        "id": "NXozIK6_Wr5U",
        "outputId": "b592e0de-6481-4551-9904-06988b9dd89c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed frame 1\n",
            "Processed frame 2\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "operands could not be broadcast together with shapes (0,4) (4,4) ",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-15-1b30d76d97ff>\u001b[0m in \u001b[0;36m<cell line: 67>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[0mframe_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[0;32mwhile\u001b[0m \u001b[0mret\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m     \u001b[0mframe_with_boxes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprev_boxes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprev_boxes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m     \u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe_with_boxes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m     \u001b[0mframe_count\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-15-1b30d76d97ff>\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(frame, prev_boxes, alpha)\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0;31m# Apply temporal smoothing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mprev_boxes\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprev_boxes\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m         \u001b[0msmoothed_boxes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0malpha\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mcurrent_boxes\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mprev_boxes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m         \u001b[0msmoothed_boxes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msmoothed_boxes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mbox\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msmoothed_boxes\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: operands could not be broadcast together with shapes (0,4) (4,4) "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchvision.transforms as T\n",
        "from PIL import Image, ImageDraw\n",
        "from torchvision.models.detection import fasterrcnn_resnet50_fpn\n",
        "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
        "\n",
        "# Load the trained model\n",
        "model = fasterrcnn_resnet50_fpn(pretrained=False)\n",
        "num_classes = 2  # Background + Spaghetti\n",
        "in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
        "model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
        "model.load_state_dict(torch.load('/content/drive/MyDrive/print fails.v1i.voc/train/spaghetti_detection_model.pth'))\n",
        "model.eval()\n",
        "\n",
        "# Define the transform to preprocess the image\n",
        "transform = T.Compose([T.ToTensor()])\n",
        "\n",
        "# Define the function to perform inference and draw bounding boxes\n",
        "def predict(image_path, output_path):\n",
        "    image = Image.open(image_path).convert(\"RGB\")\n",
        "    image_tensor = transform(image).unsqueeze(0)\n",
        "\n",
        "    # Perform inference\n",
        "    with torch.no_grad():\n",
        "        prediction = model(image_tensor)[0]\n",
        "\n",
        "    # Draw bounding boxes on the image\n",
        "    image_draw = image.copy()\n",
        "    draw = ImageDraw.Draw(image_draw)\n",
        "    for box in prediction['boxes']:\n",
        "        draw.rectangle([(box[0], box[1]), (box[2], box[3])], outline=\"red\")\n",
        "\n",
        "    # Save the image with bounding boxes\n",
        "    image_draw.save(output_path)\n",
        "\n",
        "# Run inference on an input image and save the result\n",
        "image_path = \"/content/drive/MyDrive/prob_dataset/train/1672793083-107055_jpg.rf.56edc6f514bc2354fb6e0ae2c7bdfdb1.jpg\"\n",
        "output_path = \"/content/drive/MyDrive/print fails.v1i.voc/result.jpg\"\n",
        "\n",
        "predict(image_path, output_path)\n"
      ],
      "metadata": {
        "id": "_lTJGNKQFj8F"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}